#《DEEP LEARNING》读书笔记 -- 2017年7月21日
* 5.1 Learning Algorithms
    * 机器学习中学习的意义是什么？Mitchell (1997)提出了一个概念： “A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.”
    * 5.1.1 The Task, $T$
        * 学习是指获得执行任务的能力，而机器学习任务通常被形容为如何让一个机器学习系统去执行一个example。
        * 一个example是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的feature的集合。我们通常用一个向量来表示example，其中每个元素都是一个feature。
        * 机器学习任务的大致分类分为以下几种：
            * Classification：电脑程序被要求从$k$个类型中，找出输入的是哪个类型。通常做法就是产生一个函数：$f : \mathbb{R}^{n} \to \lbrace 1,\dots,k\rbrace$，当模型$y=f(x)$时，就可以将输入转化成一个类型。还有一种是输出该类型的probability distribution，这种分类的example是目标的Regression
            * Classification with missing inputs：当提供的输入的每个度量无法保证时，Classification会变得更加具有挑战性。为了解决这个问题，我们需要使用一个简单的函数建立单一输入与输出类型间的映射。当遇到输入丢失时，不是提供一个简单的Classification函数，而是必须学习一个函数的集合。对于集合中每个函数都要有一个输入缺失的子集。我们可以学习一个probability distribution的函数集合，然后通过通过边缘化处理来分离每个类型中的输入缺失。
            * Regression：电脑程序被要求根据输入来预测出一个数字。通常的做法是产生一个函数：$f : \mathbb{R}^{n} \to \mathbb{R}$，这种类型和Classification除了输出格式不同外，其余都是相似的。
            * Transcription：机器学习系统观测一些相对非结构化表示的数据，并转录信息为离散的文本形式。
            * Machine translation：将输入一种语言的已组成的符号序列转变成另一种语言的符号序列。
            * Structured output：指输出是向量或者其他包含多个值的数据结构， 并且构成输出的这些不同元素间具有重要关系。这个范畴很大，包括Transcription和Machine translation都是这个类型。例如图像的像素级分割。
            * Anomaly detection：电脑程序可以筛出一些事件或者目标，它们中有一些标志是不寻常或者非典型的。
            * Synthesis and sampling：通过机器学习算法生成一个与训练数据相似的新的example。Synthesis and sampling在媒体应用中非常有用，可以避免艺术家大量 昂贵或者乏味费时的手动工作。
            * Imputation of missing values：机器学习算法被给于了一个新的但是其中有些项是丢失了的example。而算法就是要把这缺失的这部分预测出来。
            * Denoising：机器学习算法被给于了一张通过对clean example $x\in\mathbb{R}^{n}$进行未知损坏生成的corrupted example $\tilde{x}\in\mathbb{R}^{n}$。算法则要从corrupted example $\tilde{x}$预测出clean example $x$，更一般的来说，要预测出$p(x|\tilde{x})$的条件概率分布。
            * Density estimation or probability mass function estimation：机器学习算法学习函数$p_{\mathrm{model}}:\mathbb{R}^{n}\to\mathbb{R}$，其中$p_{\mathrm{model}}(x)$可以解释成样本采样空间的概率密度函数(如果是连续的)或者概率质量函数(如果是离散的)。要做好这样的任务，需要学习观测到数据的结构。算法必须知道什么情况下样本聚集出现，什么情况下不太可能出现。以上描述的大多数任务都要求学习算法至少能隐式地捕获概率分布的结构。密度估计可以让我们显式地捕获该分布。原则上，我们可以在该分布上计算以便解决其他任务。实际情况中，密度估计并不能够解决所有这类问题，因为在很多情况下$p(x)$是难以计算的。
        * 还有很多其他同类型或其他类型的任务。这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务，并非严格地定义机器学习任务分类。
* 5.1.2 The Performance Measure, $P$
    * 对于诸如分类、缺失输入分类和转录任务，我用来衡量的模型有：
        * accuracy：计算该模型输出正确结果的example的比例。
        * error rate：计算该模型输出错误结果的example的比例。通常我也把error rate叫做$0-1$loss，如果结果正确就是0，否则就是1。
    * 对于密度这类模型，我们一般使用不同的性能度量，来给予模型一个连续的得分。最常用的方法是输出模型在一些example上概率对数的平均值。
    * 为了确定模型实际的能力，我们还要从训练集中分离一部分数据作为test set来评估模型的性能。
    * 很多情况下，我们评估模型的性能是依照应用而定的。
    * 还有一些情况，我们知道应该度量哪些数值，但是度量它们不太现实。这种情况经常出现在密度估计中。很多最好的概率模型只能隐式地表示概率分布。在许多这类模型中，计算空间中特定点的概率是不可行的。在这些情况下，我们必须设计一个仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。