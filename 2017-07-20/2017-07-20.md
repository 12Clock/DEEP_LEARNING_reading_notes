#《DEEP LEARNING》读书笔记 -- 2017年7月20日
* 4.4 Constrained Optimization
    * 通常我们希望去找一个函数的最大值或者最小值在整个$x$的值域里面，然而有时候我们要求在一个我们所规定的范围$\mathbb{S}$内去寻找最大值或者最小值，这就是我们所知道的Constrained Optimization，而这个范围$\mathbb{S}$内的点我们称其为feasible points。
    * 如果我们使用一个恒定的步长$\epsilon$，我们可以先去梯度下降的结果，然后将它投影回$\mathbb{S}$；如果我们使用的是line search，饿哦们就可以约束线上每一个点，使其投影到约束范围内。如果可能，我们可以将整个梯度空间都投影到可行域内，这种方法更高效。
    * 一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原 始约束优化问题的解。这种方法需要创造性;优化问题之间的转换必须专门根据我们遇 到的每一种情况进行设计。
    * Karush–Kuhn–Tucker（KKT）就提出了一种对于约束问题非常通用的方法，在这里我们将先介绍一种新的函数叫做：generalized Lagrangian或者generalized Lagrange function。
        对于$\epsilon$有两部分组成--等式与不等式，我们将其描述成$\epsilon=\lbrace x|\forall i,g^{(i)}(x)=0\;and\forall j,h^{(j)}(x)\leq0\rbrace$,这里的$g^{(i)}$我们称其equality constraints，这里的$h^{(j)}$我们称其为inequality constraints。
        [$L(x,\lambda,\alpha)=f(x)+\sum_{i}\lambda_{i}g^{(i)}(x)+\sum_{j}\alpha_{j}h^{(j)}(x)$]
            其中$\lambda_{i}$和$\alpha_{j}$称作KKT multipliers，现在我就可以通过使用generalized Lagrangian的unconstrained optimization，来约束最小值，只要存在至少一个非$\infty$的feasible points。
        [$\mathrm{min}_{x}\;\mathrm{max}_{\lambda}\;\mathrm{max}_{\alpha,\alpha \geq 0}\;L(x,\lambda,\alpha)$]
        这里我们可以优化目标函数：
        [$\mathrm{min}_{x\in\mathbb{s}}\;f(x)$]
        因为当满足约定时：
        [$\mathrm{max}_{\lambda}\;\mathrm{max}_{\alpha,\alpha \geq 0}\;L(x,\lambda,\alpha)=f(x)$]
        不满足条件时：
        [$\mathrm{max}_{\lambda}\;\mathrm{max}_{\alpha,\alpha \geq 0}\;L(x,\lambda,\alpha)=\infty$]
        这些性质保证不可行点不会是最佳的，并且可惜范围内的最优点不变。等式约束对应项的符号并不重要;因为优化可以自由选择每个$\lambda_{i}$的符号，我们可以 随意将其定义为加法或减法。
    * 不等式约束特别有趣。如果$h^{(i)}(x^{*})=0$，我们就说这个约束$h^{(i)}(x^{*})$是active 的。如果约束不是active的，则有该约束的问题的解与去掉该约束的问题的解至少存在一个相同的局部解。一个不活跃约束有可能排除其他解。换句话来说我们可以通过KKT multipliers来影响这个解，如果这个约束不是active，我们可以将其对应的KKT multipliers设为0，这样这个约束就不会产生效果。
    * 我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称为KKT条件 (Karush, 1939; Kuhn and Tucker, 1951)。 这些是确定一个点是最优点的必要条件，但不一定是充分条件。这些条件是:
        * generalized Lagrangian的导数为0
        * 所有的有关于x和KKT multipliers的约束都满足。
        * 不等式约束显示的“complementary slackness”：$\alpha \odot h(x)=0$
* 4.5 Example: Linear Least Squares
    * 假设我们寻找x的最小值：
        [$f(x)=\frac{1}{2}||Ax-b||_{2}^{2}$]
        这里我们不使用线性代数，我们使用基于梯度的优化方法：
        [$\nabla_{x}f(x)=A^{T}(Ax-b)=A^{T}Ax-A^{T}b$]
        然后通过迭代更新$x$，使得$x$收敛到最小值。
    * 这里我们也可以用Newton's method，因为函数时二次的所以我们可以通过Newton's method一步收敛到全局最小值，但是收到$x^{T}x \leq 1$的约束，我们要使用Lagrangian：
        [$L(x,\lambda)=f(x)+\lambda(x^{T}x-1)$]
        现在，我们解决一下问题：
        [$\mathrm{min}_{x}\;\mathrm{max}_{\lambda,\lambda \geq 0}\;L(x,\lambda)$]
        如果这个点是feasible，我们可以用Moore-Penrose pseudoinverse：$x=A^{+}b$来求解。否者我们需要找到一个约束是active的解：
        [$A^{T}Ax-A^{T}b+2\lambda x=0$]
        其解便等于：
        [$x=(A^{T}A+2\lambda I)^{-1}A^{T}b$]
        $\lambda$必须使结果服从这个约束，我们可以关于$\lambda$使用gradient ascent来求解x：
        [$\frac{\partial}{\partial\lambda}L(x,\lambda)=x^{T}x-1$]
        但这里的x的范数超过1时，导数才为正，所以遵照导数上升才能增加Lagrangian中对应的$\lambda$。因为$x^{T}x$的惩罚系数增加了，求解关于$x$的线性方程现在将得到具有较小范数的解。求解线性方程和调整$\lambda$的过程将一直持续到具有正确的范数并且关于$\lambda$的导数为0。








